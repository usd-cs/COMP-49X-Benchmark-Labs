{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Dependencies\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Global Init Variables\n",
    "GET_WEATHER = False # Flag to control weather data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and clean observations data from iNaturalist\n",
    "observations_df = pd.read_csv('observations.csv')\n",
    "observations_df = observations_df.dropna() # Drop rows with missing values\n",
    "\n",
    "# Display initial data overview\n",
    "print(\"Initial Data Overview:\")\n",
    "print(\"=====================\")\n",
    "observations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parse Datetime into relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse date_string and extract year/month/day\n",
    "def extract_date_components(date_string):\n",
    "    import datetime\n",
    "    # Parse ISO format date string\n",
    "    dt = datetime.datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n",
    "    return dt.year, dt.month, dt.day\n",
    "\n",
    "# Extract relevant data for true observations\n",
    "lons = []\n",
    "lats = []\n",
    "years = []\n",
    "months = []\n",
    "days = []\n",
    "\n",
    "for _, row in observations_df.iterrows():\n",
    "    # Parse the coordinates string into a list of floats\n",
    "    coords = eval(row['coordinates'])\n",
    "    lons.append(coords[0])\n",
    "    lats.append(coords[1])\n",
    "    \n",
    "    # Extract the date components from the date_string\n",
    "    year, month, day = extract_date_components(row['date_string'])\n",
    "    years.append(year)\n",
    "    months.append(month)\n",
    "    days.append(day)\n",
    "    \n",
    "# Apply parsed data to the observations_df\n",
    "observations_df['longitude'] = lons\n",
    "observations_df['latitude'] = lats\n",
    "observations_df['year'] = years\n",
    "observations_df['month'] = months\n",
    "observations_df['day'] = days\n",
    "\n",
    "# Calculate day of year from year, month, and day\n",
    "observations_df['day_of_year'] = observations_df.apply(\n",
    "    lambda row: datetime.datetime(row['year'], row['month'], row['day']).timetuple().tm_yday, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display updated data overview\n",
    "print(\"Updated Data Overview:\")\n",
    "print(\"=====================\")\n",
    "observations_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Gather Historical Training Data from NASA POWER DAV API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_weather(lon, lat, day, month, year, param_list):\n",
    "    \"\"\"\n",
    "    Get NASA weather data at given coordinates for specified date range.\n",
    "    Right now we are using the prior 3 weeks (21 days) for each point.\n",
    "    \n",
    "    Parameters:\n",
    "        lon (float): Longitude coordinate\n",
    "        lat (float): Latitude coordinate\n",
    "        day (int): Day of the month\n",
    "        month (int): Month of the year\n",
    "        year (int): Year\n",
    "        param_list (list): List of weather parameters to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing requested weather parameters and values\n",
    "    \"\"\"\n",
    "    url = \"https://power.larc.nasa.gov/api/temporal/hourly/point\"\n",
    "    \n",
    "    month_str = str(month).zfill(2)\n",
    "    day_str = str(day).zfill(2)\n",
    "    start_date = f\"{year}-{month_str}-{day_str}\"\n",
    "    \n",
    "    start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\") \n",
    "    end_date = start_date\n",
    "    start_date -= datetime.timedelta(days=21)\n",
    "    \n",
    "    start_date = start_date.strftime(\"%Y%m%d\")\n",
    "    end_date = end_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    parameters = {\n",
    "        \"parameters\": \",\".join(param_list),\n",
    "        \"community\": \"AG\",\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start\": start_date,\n",
    "        \"end\": end_date,\n",
    "        \"format\": \"JSON\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=parameters)\n",
    "        data = response.json()\n",
    "        curr_data = {param: data['properties']['parameter'][param] for param in param_list}\n",
    "        return curr_data\n",
    "    except:\n",
    "        print(f\"Error getting data for {year}-{month_str}-{day_str} at {lat},{lon}\")\n",
    "        return None\n",
    "\n",
    "# Collect weather data if enabled\n",
    "if GET_WEATHER:\n",
    "    get_general_weather()  # Function implementation moved to separate cell for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GET_WEATHER is True, fetch weather data for each observation, save to weather_data.csv\n",
    "\n",
    "if GET_WEATHER:\n",
    "    \"\"\"\n",
    "    These are the updated parameters that are also available on Open Meteo (imortant for integrating a user forecast)\n",
    "    T2M\tTemperature at 2 Meters\n",
    "    RH2M\tRelative Humidity at 2 Meters\n",
    "    T2MDEW\tDew/Frost Point at 2 Meters\n",
    "    PRECTOTCORR\tPrecipitation Corrected\n",
    "    TSOIL1\tSoil Temperatures Layer 1\n",
    "    T2MWET\tWet Bulb Temperature at 2 Meters\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    dataframe = pd.DataFrame(columns=['Datetime','id', 'Temperature', 'Humidity', 'Wind Speed', 'Dew/Frost Point', 'Wet Bulb Temperature', 'Specific Humidity'])\n",
    "\n",
    "    param_list = ['T2M', 'RH2M', 'T2MDEW', 'PRECTOTCORR', 'TSOIL1', 'T2MWET']\n",
    "\n",
    "    for i, row in observations_df.iterrows():\n",
    "        weather_data = get_general_weather(row['longitude'], row['latitude'], row['day'], row['month'], row['year'], param_list)\n",
    "        if weather_data is None:\n",
    "            for j in range(3):\n",
    "                weather_data = get_general_weather(row['longitude'], row['latitude'], row['day'], row['month'], row['year'], param_list)\n",
    "                if weather_data is not None:\n",
    "                    break\n",
    "        if weather_data is None:    \n",
    "            continue\n",
    "        dates = list(weather_data['T2M'].keys())\n",
    "        id_vals = [row['id']] * len(dates)\n",
    "        temp_values = list(weather_data['T2M'].values())\n",
    "        humidity_values = list(weather_data['RH2M'].values())\n",
    "        dew_frost_values = list(weather_data['T2MDEW'].values())\n",
    "        precipitation_values = list(weather_data['PRECTOTCORR'].values())\n",
    "        soil_temperature_values = list(weather_data['TSOIL1'].values())\n",
    "        wet_bulb_values = list(weather_data['T2MWET'].values())\n",
    "        curr_df = pd.DataFrame({\n",
    "            'Datetime': dates,\n",
    "            'id': id_vals,\n",
    "            'Temperature': temp_values,\n",
    "            'Humidity': humidity_values,\n",
    "            'Precipitation': precipitation_values,\n",
    "            'Dew/Frost Point': dew_frost_values,\n",
    "            'Wet Bulb Temperature': wet_bulb_values,\n",
    "            'Soil Temperature': soil_temperature_values\n",
    "        })\n",
    "        \n",
    "        weather_vars = ['Temperature', 'Humidity', 'Precipitation', 'Dew/Frost Point', 'Wet Bulb Temperature', 'Soil Temperature']\n",
    "        \n",
    "        dataframe = pd.concat([dataframe, curr_df], ignore_index=True)\n",
    "        # Print progress every 10 rows\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i}\")\n",
    "        \n",
    "    dataframe.to_csv('weather_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Feature Engineering: using NASA weather data, calculate statistics to aggregate hourly weather conditions according to different time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather data if not already loaded\n",
    "from datetime import datetime, timedelta\n",
    "weather_df = pd.read_csv('weather_data.csv')\n",
    "\n",
    "# convert datetime to same format as observations_df\n",
    "weather_df['Datetime'] = pd.to_datetime(weather_df['Datetime'], format='%Y%m%d%H')\n",
    "weather_df['Date'] = weather_df['Datetime'].dt.date\n",
    "\n",
    "# Use same format as weather_df\n",
    "observations_df['Date'] = pd.to_datetime(observations_df[['year', 'month', 'day']])\n",
    "\n",
    "# Group by id and Date to find all unique days per ID\n",
    "day_indices = weather_df.groupby(['id', 'Date']).first().reset_index()\n",
    "\n",
    "# make dict of observation ID with original date\n",
    "observation_dates = {}\n",
    "for _, row in observations_df.iterrows():\n",
    "    observation_dates[row['id']] = row['Date']\n",
    "\n",
    "# Calculate difference between observation date and weather date\n",
    "def get_relative_day(row):\n",
    "    obs_date = observation_dates.get(row['id'])\n",
    "    this_date = pd.to_datetime(row['Date'])\n",
    "    difference = (this_date - obs_date).days\n",
    "    return difference\n",
    "\n",
    "# Apply the function to get relative day\n",
    "day_indices['RelativeDay'] = day_indices.apply(get_relative_day, axis=1)\n",
    "\n",
    "# Add relative day to weather_df\n",
    "day_map = day_indices.set_index(['id', 'Date'])['RelativeDay']\n",
    "weather_df['RelativeDay'] = weather_df.set_index(['id', 'Date']).index.map(day_map)\n",
    "\n",
    "# Variables to calculate aggregations for\n",
    "weather_vars = ['Temperature', 'Humidity', 'Precipitation', 'Dew/Frost Point', 'Wet Bulb Temperature', 'Soil Temperature']\n",
    "\n",
    "# Time windows to aggregate over (days before and after the observation day)\n",
    "time_windows = {\n",
    "    'day_of': (0, 0),\n",
    "    'day_before_after': (-1, 0),\n",
    "    'two_days': (-2, 0),\n",
    "    'three_days': (-3, 0),\n",
    "    'one_week': (-7, 0),\n",
    "    'two_week': (-14, 0),\n",
    "    ## 'three_week': (-21, 0)\n",
    "}\n",
    "\n",
    "# Function to calculate aggregates given ID and time window\n",
    "def calculate_aggregates(weather_subset, var_name, window_name, start_day, end_day):\n",
    "    # Filter data for given time window\n",
    "    window_data = weather_subset[(weather_subset['RelativeDay'] >= start_day) & (weather_subset['RelativeDay'] <= end_day)]\n",
    "    \n",
    "    # Calculate aggregate and return as a series\n",
    "    result = pd.Series({\n",
    "        f\"{var_name}_{window_name}_mean\": window_data[var_name].mean(),\n",
    "        f\"{var_name}_{window_name}_min\": window_data[var_name].min(),\n",
    "        f\"{var_name}_{window_name}_max\": window_data[var_name].max(),\n",
    "        f\"{var_name}_{window_name}_median\": window_data[var_name].median()\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create empty DataFrame to store results\n",
    "all_features = []\n",
    "result_rows = []\n",
    "\n",
    "# For each observation row, calculate all aggregate weather data\n",
    "for id_val in observations_df['id'].unique():\n",
    "    id_weather = weather_df[weather_df['id'] == id_val]\n",
    "        \n",
    "    # Dictionary to store aggregated values for this ID\n",
    "    id_features = {'id': id_val}\n",
    "    \n",
    "    # Iterate over all weather data variables\n",
    "    for var in weather_vars:\n",
    "        # iterate over all items in time_windows\n",
    "        for window_name, (start_day, end_day) in time_windows.items():\n",
    "            aggs = calculate_aggregates(id_weather, var, window_name, start_day, end_day)\n",
    "            id_features.update(aggs)\n",
    "            \n",
    "            # Add feature names to our list (only once)\n",
    "            if id_val == observations_df['id'].unique()[0]:\n",
    "                all_features.extend(aggs.index.tolist())\n",
    "    \n",
    "    result_rows.append(id_features)\n",
    "\n",
    "# Create DataFrame from results\n",
    "aggregated_features = pd.DataFrame(result_rows)\n",
    "\n",
    "# Merge with the original observations dataframe\n",
    "observations_df = observations_df.merge(aggregated_features, on='id', how='left')\n",
    "\n",
    "# Create list of feature names\n",
    "weather_feature_names = all_features\n",
    "\n",
    "print(f\"Created {len(weather_feature_names)}  features\")\n",
    "\n",
    "# Check: display updated data overview that now has statistics over each time window\n",
    "print(\"Updated Data w Statistics Overview:\")\n",
    "print(\"=====================\")\n",
    "observations_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build a classification model using this updated data frame!\n",
    "\n",
    "5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "features = weather_feature_names + ['day_of_year']\n",
    "target = 'PowderyMildew'\n",
    "\n",
    "X = observations_df[features]\n",
    "y = observations_df[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Create and display confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix with labels\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                             display_labels=[False, True])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Feature Importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Select top 20 features\n",
    "top_20_features = feature_importance_df.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=top_20_features)\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Compare Random Forest to other model options in scikitlearn. Which is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional models and metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set environment variable to avoid KNN warning\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(os.cpu_count())\n",
    "\n",
    "# First, let's check where the NaN values are\n",
    "print(\"Columns with missing values:\")\n",
    "print(X.isna().sum()[X.isna().sum() > 0])\n",
    "\n",
    "# Define preprocessing and model evaluation pipeline\n",
    "def create_pipeline(model):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing steps and the model.\n",
    "    \"\"\"\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with mean\n",
    "        ('scaler', StandardScaler()),                 # Scale features\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "# Define a function to evaluate models\n",
    "def evaluate_model(pipeline, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate a model using multiple metrics and cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    # Train the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1-score': f1,\n",
    "        'CV Mean Score': cv_scores.mean(),\n",
    "        'CV Std Dev': cv_scores.std()\n",
    "    }\n",
    "\n",
    "# Define base models to test\n",
    "base_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=300, max_depth=3, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        n_estimators=300, \n",
    "        random_state=42,\n",
    "        algorithm='SAMME'  # Changed from default SAMME.R to avoid deprecation warning\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=300, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(\n",
    "        n_neighbors=5,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    ),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "# Create pipelines for each model\n",
    "models = {name: create_pipeline(model) for name, model in base_models.items()}\n",
    "\n",
    "# Prepare data (assuming X and y are already defined as before)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    result = evaluate_model(pipeline, X_train, X_test, y_train, y_test, name)\n",
    "    results.append(result)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=['Recall', 'Accuracy'], ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(\"========================\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['Accuracy', 'Recall', 'Precision', 'F1-score']\n",
    "results_df_plot = results_df.melt(id_vars=['Model'], \n",
    "                                 value_vars=metrics, \n",
    "                                 var_name='Metric', \n",
    "                                 value_name='Score')\n",
    "\n",
    "sns.barplot(x='Model', y='Score', hue='Metric', data=results_df_plot)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model based on recall and accuracy\n",
    "best_model_recall = results_df.iloc[0]\n",
    "print(\"\\nBest Model (prioritizing recall):\")\n",
    "print(\"================================\")\n",
    "print(f\"Model: {best_model_recall['Model']}\")\n",
    "print(f\"Recall: {best_model_recall['Recall']:.4f}\")\n",
    "print(f\"Accuracy: {best_model_recall['Accuracy']:.4f}\")\n",
    "print(f\"F1-score: {best_model_recall['F1-score']:.4f}\")\n",
    "\n",
    "# Detailed analysis of best model\n",
    "best_model_name = best_model_recall['Model']\n",
    "best_pipeline = models[best_model_name]\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\nDetailed Classification Report for Best Model:\")\n",
    "print(\"===========================================\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix for best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# If the best model supports feature importance, plot it\n",
    "best_model = best_pipeline.named_steps['model']\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(10))\n",
    "    plt.title(f'Top 10 Feature Importance - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. For this dataset, KNN is the best balance of accuracy and recall. Let's save it as a seperate model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
